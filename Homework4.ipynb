{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonsoloo/PlanetTerp-Professor-Review-Rating-Predictor/blob/main/Homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZmmA9xqRcT2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VjN4TWzFCl3R"
      },
      "outputs": [],
      "source": [
        "!pip install planetterp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r1DdKe2zEWaD"
      },
      "outputs": [],
      "source": [
        "import planetterp\n",
        "\n",
        "\n",
        "prof = planetterp.professor(name=\"Shin Song\", reviews=\"true\")\n",
        "print(prof)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIY3CULNGUng"
      },
      "outputs": [],
      "source": [
        "prof[\"reviews\"]\n",
        "prof['average_rating']\n",
        "prof['name']\n",
        "prof['slug']\n",
        "prof['type']\n",
        "prof['courses']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KtLbfIm9HSS0"
      },
      "outputs": [],
      "source": [
        "#Picking Professors\n",
        "\n",
        "professors = [\n",
        "    \"Nelson Padua-Perez\",\n",
        "    \"Marvin Jones\",\n",
        "    \"Clyde Kruskal\",\n",
        "    \"Shin Song\",\n",
        "    \"Ilchul Yoon\"\n",
        "]\n",
        "\n",
        "info = []\n",
        "\n",
        "for prof in professors:\n",
        "    data = planetterp.professor(name=prof, reviews=True)\n",
        "    info.append(data)\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "un9HmORDNk0I"
      },
      "outputs": [],
      "source": [
        "# Getting Stars and Reviews\n",
        "\n",
        "reviews = []\n",
        "\n",
        "for prof in professors:\n",
        "    data = planetterp.professor(name=prof, reviews=True)\n",
        "    for r in data[\"reviews\"]:\n",
        "        overview = r.get(\"review\")\n",
        "        stars = r.get(\"rating\")\n",
        "        if overview and stars is not None:\n",
        "            reviews.append({\n",
        "                \"professor\": prof,\n",
        "                \"overview\": overview,\n",
        "                \"star_count\": int(stars)\n",
        "            })\n",
        "print(reviews)\n",
        "\n",
        "overviews = [r[\"overview\"] for r in reviews]\n",
        "star_count = [r[\"star_count\"] for r in reviews]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLlO7DJcJ6RT"
      },
      "outputs": [],
      "source": [
        "lengths = [len(tokenizer(r).input_ids) for r in overviews]\n",
        "\n",
        "print(\"Median length:\", np.median(lengths))\n",
        "print(\"95th percentile:\", np.percentile(lengths, 95))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWD6lXMDRjII"
      },
      "outputs": [],
      "source": [
        "#Train Test Splits\n",
        "train_overviews, test_overviews, train_star_count, test_star_count = train_test_split(\n",
        "    overviews,\n",
        "    star_count,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        ")\n",
        "train_data = Dataset.from_dict({\n",
        "    \"overview\": train_overviews,\n",
        "    \"star_count\": train_star_count,\n",
        "})\n",
        "\n",
        "test_data = Dataset.from_dict({\n",
        "    \"overview\": test_overviews,\n",
        "    \"star_count\": test_star_count,\n",
        "})\n",
        "\n",
        "def add_label(example):\n",
        "    example[\"label\"] = example[\"star_count\"] - 1\n",
        "    return example\n",
        "\n",
        "train_data = train_data.map(add_label)\n",
        "test_data = test_data.map(add_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "277RAprW-Z2A"
      },
      "outputs": [],
      "source": [
        "#Tokenizing\n",
        "\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"overview\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "train_tokenized = train_data.map(tokenize_batch, batched=True)\n",
        "test_tokenized  = test_data.map(tokenize_batch, batched=True)\n",
        "train_tokenized = train_tokenized.remove_columns([\"overview\", \"star_count\"])\n",
        "test_tokenized  = test_tokenized.remove_columns([\"overview\", \"star_count\"])\n",
        "\n",
        "train_tokenized.set_format(\"torch\")\n",
        "test_tokenized.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJM6APNa-CWi"
      },
      "outputs": [],
      "source": [
        "#Transformer\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=5\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./planetterp_distilbert\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtHJYZJUBs_b"
      },
      "outputs": [],
      "source": [
        "#Predict Stars\n",
        "\n",
        "pred_output = trainer.predict(test_tokenized)\n",
        "pred_labels = np.argmax(pred_output.predictions, axis=-1)\n",
        "pred_star_count = pred_labels + 1\n",
        "true_star_count = np.array(test_data[\"star_count\"])\n",
        "accuracy = np.mean(pred_star_count == true_star_count)\n",
        "print(\"Exact star prediction accuracy:\", accuracy)\n",
        "for i in range(10):\n",
        "    print(\"Review:\", test_data[i][\"overview\"][:200], \"...\")\n",
        "    print(\"Actual stars:\", int(true_star_count[i]))\n",
        "    print(\"Predicted stars:\", int(pred_star_count[i]))\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for Slide Visuals\n",
        "import matplotlib.pyplot as plt\n",
        "pred_output = trainer.predict(test_tokenized)\n",
        "pred_labels = np.argmax(pred_output.predictions, axis=-1) # 0–4\n",
        "pred_star_count = pred_labels + 1 # 1–5\n",
        "true_star_count = np.array(test_data[\"star_count\"])\n",
        "accuracy = np.mean(pred_star_count == true_star_count)\n",
        "print(\"Accuracy (exact star match):\", accuracy)\n",
        "random_accuracy = 1/5 # 5 star classes\n",
        "plt.figure()\n",
        "plt.bar([\"Random baseline\", \"Model\"], [random_accuracy, accuracy])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "686IdB8A6MG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slide Visuals 2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "labels = [1, 2, 3, 4, 5]\n",
        "cm = confusion_matrix(true_star_count, pred_star_count, labels=labels)\n",
        "plt.figure()\n",
        "plt.imshow(cm, interpolation=\"nearest\")\n",
        "plt.title(\"Confusion Matrix (rows=true, cols=predicted)\")\n",
        "plt.xlabel(\"Predicted Stars\")\n",
        "plt.ylabel(\"True Stars\")\n",
        "plt.xticks(range(len(labels)), labels)\n",
        "plt.yticks(range(len(labels)), labels)\n",
        "for i in range(cm.shape[0]):\n",
        "for j in range(cm.shape[1]):\n",
        "plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ZXkWA846MTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Slide Visuals 3\n",
        "true_counts = [int(np.sum(true_star_count == s)) for s in labels]\n",
        "pred_counts = [int(np.sum(pred_star_count == s)) for s in labels]\n",
        "x = np.arange(len(labels))\n",
        "width = 0.38\n",
        "plt.figure()\n",
        "plt.bar(x - width/2, true_counts, width, label=\"True\")\n",
        "plt.bar(x + width/2, pred_counts, width, label=\"Predicted\")\n",
        "plt.xticks(x, labels)\n",
        "plt.xlabel(\"Stars\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Star Distribution (True vs Predicted)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gr2pY42p6MgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Slide Visuals 4\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "comparison_df = pd.DataFrame({\n",
        "\"overview\": test_data[\"overview\"],\n",
        "\"true_stars\": true_star_count.astype(int),\n",
        "\"pred_stars\": pred_star_count.astype(int),\n",
        "})\n",
        "comparison_df[\"correct\"] = (comparison_df[\"true_stars\"] == comparison_df[\"pred_stars\"])\n",
        "display(comparison_df.head(20))"
      ],
      "metadata": {
        "id": "JxxBn4WV6M1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPe6Ur33HTs2"
      },
      "outputs": [],
      "source": [
        "#MAX Piazza Code\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "\n",
        "model_name,\n",
        "\n",
        "device_map=\"auto\", # uses GPU if available, CPU if not\n",
        "\n",
        "torch_dtype=torch.float32 # safest, works everywhere\n",
        "\n",
        ")\n",
        "\n",
        "prompt = \"Give a single number of stars, between 1 and 5, that you think this review is assigning: This guy fucking sucks, worst teacher hands down.\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "output_tokens = model.generate(\n",
        "\n",
        "**inputs,\n",
        "\n",
        "max_new_tokens=150,\n",
        "\n",
        "do_sample=True,\n",
        "\n",
        "temperature=0.7,\n",
        "\n",
        "top_p=0.9,\n",
        "\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True)) \"\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNSWYV2nl2ES5FiqziOs2jL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}